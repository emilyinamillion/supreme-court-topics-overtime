{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Notebook\n",
    "\n",
    "I originally had this notebook as part of the initial modeling notebook, but split it out to make everything a bit more clear. Additionally, I've learned a lot about text preprocessing since originally doing this project, so I made these functions a bit more efficient as well. \n",
    "\n",
    "As a note about stopwords if you've never done NLP before - there's no one size fits all stopwords subsitution list for knowledge of your domain and which words should be excluded (or kept). Creating a powerful stopwords list is an interative process and requires a lot of stopchecking of your actual data to do it right.\n",
    "\n",
    "## Text preprocessing process\n",
    "1. Removing state names\n",
    "2. Removing case names\n",
    "3. Removing common stopwords (for example, \"the\" isn't a useful word)\n",
    "4. Removing people's names (loading the baby name dataset from sklearn)\n",
    "5. Removing day of the week, month names - this throws off our model into thinking we care about period of time\n",
    "6. Stripping non-words (lots of numbers referencing other cases - another interesting project could be keeping ONLY nums)\n",
    "7. Lemmatizing (getting the root of a word - ie run out of running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = nltk.corpus.names\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "male_names = [w.lower() for w in male_names]\n",
    "male_names_plur = [(w.lower() + \"s\") for w in male_names]\n",
    "female_names_plur = [(w.lower() + \"s\") for w in female_names]\n",
    "female_names = [w.lower() for w in female_names]\n",
    "casenames = list(pd.read_csv(\"casetitles.csv\",encoding = 'iso-8859-1'))\n",
    "statenames = list(pd.read_csv(\"statenames.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homespun_words = ['join', 'seek', 'ginnane', 'kestenbaum', 'hummel', 'loevinger', 'note', 'curiam', 'mosk', 'pd', \\\n",
    "                'paxton', 'rhino', 'buchsbaum', 'hirshowitz', 'misc', 'assistant', 'whereon', 'dismiss', 'sod', \\\n",
    "                'vote', 'present', 'entire', 'frankfurter', 'ante', 'leave', 'concur', 'entire', 'mootness', \\\n",
    "                'track', 'constitution', 'jj', 'blackmun', 'rehnquist', 'amici,sup', 'rep', 'stat', 'messes', \\\n",
    "                'like', 'rev', 'trans', 'bra', 'teller', 'vii', 'erisa', 'usca', 'annas', 'lead', 'cf', 'cca', \\\n",
    "                'fsupp', 'afdc', 'amicus', 'ante', 'orrick', 'kansa', 'pd', 'foth', 'stucky', 'aver',\"united\", \\\n",
    "                \"may\", \"argued\", \"argue\", \"decide\", \"rptr\", \"nervine\", \"pp\",\"fd\" ,\"june\", \"july\", \\\n",
    "                \"august\", \"september\", \"october\", \"november\", \"states\", \"ca\", \"joyce\", \"certiorari\", \"december\",\\\n",
    "                \"january\", \"february\", \"march\", \"april\", \"writ\", \"supreme court\", \"court\", \"dissent\", \\\n",
    "                \"opinion\", \"footnote\",\"brief\", \"decision\", \"member\", \"curiam\", \"dismiss\", \"note\", \"affirm\", \\\n",
    "                \"question\", \"usc\", \"file\"]\n",
    "\n",
    "STOPLIST = set(stopwords.words('english') + list(homespun_words) + list(ENGLISH_STOP_WORDS) \\\n",
    "               + list(statenames) + list(casenames) + list(female_names) + list(male_names) + \\ \n",
    "               list(female_names_plur) + list(male_names_plur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaner - including stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPLIST = set(list(stopwords.words('english')) + list(sub_list) + list(ENGLISH_STOP_WORDS))\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    separators = [\"\\xa0\\xa0\\xa0\\xa0\", \"\\r\", \"\\n\", \"\\t\", \"n't\", \"'m\", \"'ll\", '[^a-z ]']\n",
    "    for i in separators:\n",
    "        sample = re.sub(i, \" \", sample.lower())\n",
    "        \n",
    "    ## get the tokens using spaCy - this makes it possible to lemmatize the words\n",
    "    tokens = parser(sample)\n",
    "    tokens = [tok.lemma_.strip() for tok in tokens]\n",
    "\n",
    "    ## apply our stoplist\n",
    "    return [tok for tok in tokens if len(tok) != 1 and tok not in STOPLIST]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing first 300 words\n",
    "\n",
    "### Supreme Court text is extremely long\n",
    "I found through several iterations that the length of the text is often a detriment to getting the gist of the document. Each page I extracted with beautifulsoup starts with a case description, so I decided to take the first 300 (POST PROCESSED) words for each. This improved model performance significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_list[\"lem\"] = doc_list.case.apply(text_processing)\n",
    "doc_list.to_pickle(\"full_proj_lemmatized.pickle\") ## to be used in model selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
